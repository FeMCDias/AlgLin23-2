{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 6: \"Jogador de forca\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: A primeira letra escolhida sempre é a letra \"a\". Qual a entropia associada a essa escolha?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropia é uma medida da incerteza ou surpresa associada a um conjunto de possíveis resultados. No contexto do jogo de forca, a entropia associada à escolha da primeira letra, como a letra \"a\", pode ser entendida como a medida de quão surpreendente é essa escolha em relação às palavras possíveis.\n",
    "\n",
    "A entropia (H) para a letra \"a\" pode ser calculada usando a seguinte fórmula:\n",
    "\n",
    "$H = - p(a) \\log_2 p(a) - p(- a) \\log_2 p(-a)$\n",
    "\n",
    "onde $ p(a) $ é a probabilidade de a letra \"a\" estar presente em uma palavra aleatória e $ p(-a) $ é a probabilidade de a letra \"a\" não estar presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas:  245366\n",
      "Total de linhas com a:  205828\n",
      "Entropia de A:  0.6370271453820535\n",
      "Probabilidade de A:  0.8388611299038987\n",
      "Probabilidade de não A:  0.16113887009610128\n"
     ]
    }
   ],
   "source": [
    "# read br-sem-acentos.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "counter_a = 0\n",
    "counter_total = 0\n",
    "with open('br-sem-acentos.txt', 'r') as arquivo:\n",
    "    for linha in arquivo:\n",
    "        if 'a' in linha:\n",
    "            counter_a += 1\n",
    "        counter_total += 1\n",
    "\n",
    "print('Total de linhas: ', counter_total)\n",
    "print('Total de linhas com a: ', counter_a)\n",
    "\n",
    "# Entropia de A\n",
    "p_a = counter_a / counter_total\n",
    "p_not_a = 1 - p_a\n",
    "entropia_a = -p_a * np.log2(p_a) - p_not_a * np.log2(p_not_a)\n",
    "print('Entropia de A: ', entropia_a)\n",
    "\n",
    "print(\"Probabilidade de A: \", p_a)\n",
    "print(\"Probabilidade de não A: \", p_not_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Qual seria a entropia associada a escolher qualquer outra letra (a sua escolha) como o primeiro chute? Como as entropias das duas letras (\"a\" e a escolhida) se relacionam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escolhermos outra letra, como \"b\", a entropia será calculada de maneira similar:\n",
    "\n",
    "$H = - p(b) \\log_2 p(b) - p(\\neg b) \\log_2 p(\\neg b)$\n",
    "\n",
    "A relação entre as entropias da letra \"a\" e outra letra escolhida dependerá de suas frequências relativas no conjunto de palavras. Se \"a\" for mais comum que \"b\", então a entropia associada a \"a\" será menor, indicando que sua presença é menos surpreendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas:  245366\n",
      "Total de linhas com b:  29695\n",
      "Entropia de B:  0.5322951387476988\n",
      "Probabilidade de B:  0.12102328766006701\n",
      "Probabilidade de não B:  0.878976712339933\n"
     ]
    }
   ],
   "source": [
    "counter_b = 0\n",
    "with open('br-sem-acentos.txt', 'r') as arquivo:\n",
    "    for linha in arquivo:\n",
    "        if 'b' in linha:\n",
    "            counter_b += 1\n",
    "\n",
    "print('Total de linhas: ', counter_total)\n",
    "print('Total de linhas com b: ', counter_b)\n",
    "\n",
    "# Entropia de B\n",
    "\n",
    "p_b = counter_b / counter_total\n",
    "p_not_b = 1 - p_b\n",
    "entropia_b = -p_b * np.log2(p_b) - p_not_b * np.log2(p_not_b)\n",
    "print('Entropia de B: ', entropia_b)\n",
    "print(\"Probabilidade de B: \", p_b)\n",
    "print(\"Probabilidade de não B: \", p_not_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia baixa:  0.46899559358928117\n",
      "Entropia alta:  1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "low_entropy = entropy([0.9, 0.1], base=2)\n",
    "print('Entropia baixa: ', low_entropy)\n",
    "\n",
    "high_entropy = entropy([0.5, 0.5], base=2)\n",
    "print('Entropia alta: ', high_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em qualquer sistema, se existe uma alta probabilidade de uma coisa e uma baixa de outra, esse sistema tem baixa entropia. Por outro lado, se a probabilidade de ambas as coisas for igual, o sistema terá alta entropia.\n",
    "\n",
    "Em ambos os sistemas de uma palavra ter um a e ter um b, eles tem baixa entropia. Porém! No sistema de uma palavra ter um a, a probabilidade de ter um a é gigante, 0.83. No sistema de uma palavra ter um b, a probabilidade de ter um b é 0.12. Ou seja, mesmo que os dois sistemas tem baixa entropia, essa alta entropia significa coisas completamente diferentes.\n",
    "\n",
    "Baixa entropia no sistema A: É super não surpreendente uma palavra qualquer ter um a. \n",
    "Baixa entropia no sistema B: É super não surpreendente uma palavra qualquer não ter um b -> É surpeer surpreendente uma palavra qualquer ter um b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: A análise dos principais casos de erro apresentada está correta? Forneça uma justificativa baseada em dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar se a análise dos casos de erro está correta, devemos incluir:\n",
    "\n",
    "- Frequência de erros associados a cada letra.\n",
    "- Padrões nas palavras que frequentemente levam a erros.\n",
    "- Comparação entre a eficácia das estratégias de escolha de letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adelgacou', 6.475029240471162),\n",
       " ('coexista', 6.428867702832918),\n",
       " ('golpeara', 5.488307334463556),\n",
       " ('inegavel', 5.437505196472681),\n",
       " ('atalharao', 4.700198805811467),\n",
       " ('pulavas', 4.578312261438134),\n",
       " ('abalizas', 4.258645753828666),\n",
       " ('arcabuz', 4.221616840560879),\n",
       " ('falho', 3.359657758879701),\n",
       " ('teste', 2.6953713041931957),\n",
       " ('oi', 1.9477075944493865)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Função para calcular a entropia de uma letra no dicionário\n",
    "def calcular_entropia(letra, dicionario):\n",
    "    total_palavras = len(dicionario)\n",
    "    palavras_com_letra = sum(letra in palavra for palavra in dicionario)\n",
    "    p_letra = palavras_com_letra / total_palavras\n",
    "    p_nao_letra = 1 - p_letra\n",
    "\n",
    "    entropia = 0\n",
    "    if p_letra > 0:\n",
    "        entropia -= p_letra * math.log2(p_letra)\n",
    "    if p_nao_letra > 0:\n",
    "        entropia -= p_nao_letra * math.log2(p_nao_letra)\n",
    "    \n",
    "    return entropia\n",
    "\n",
    "dicionario_path = 'br-sem-acentos.txt'\n",
    "with open(dicionario_path, 'r', encoding='utf-8') as file:\n",
    "    dicionario = file.read().splitlines()\n",
    "\n",
    "alfabeto = 'abcdefghijklmnopqrstuvwxyz'\n",
    "entropia_letras = {letra: calcular_entropia(letra, dicionario) for letra in alfabeto}\n",
    "\n",
    "def calcular_entropia_palavra(palavra, entropia_letras):\n",
    "    letras_unicas = set(palavra)\n",
    "    entropia_total = sum(entropia_letras[letra] for letra in letras_unicas if letra in entropia_letras)\n",
    "    return entropia_total\n",
    "\n",
    "casos_de_erro = [\"inegavel\", \"atalharao\", \"falho\", \"arcabuz\", \"adelgacou\", \"golpeara\", \"abalizas\", \"coexista\", \"pulavas\", \"teste\", \"oi\"]\n",
    "\n",
    "entropia_casos_de_erro = {palavra: calcular_entropia_palavra(palavra, entropia_letras) for palavra in casos_de_erro}\n",
    "entropia_casos_de_erro_sorted = sorted(entropia_casos_de_erro.items(), key=lambda x: x[1], reverse=True)\n",
    "entropia_casos_de_erro_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionei duas palavras simples para os casos de erro: \"Teste\" e \"Oi\". Fiz isso para mostrar a entropia de uma palavra simples.\n",
    "\n",
    "Como conseguimos ver acima, nos nossos casos de erro, existem varias palavras com altíssimas entropias, como adelgacou e coexista.\n",
    "\n",
    "Porém, também tem palabtas com entropias altas mas não astronomicas, como abalizas e arcabuz. \n",
    "\n",
    "Em cima disso, a palavra \"falho\" tem entropia baixa, e persiste nos casos de erro.\n",
    "\n",
    "Diria que os casos de erro, por maior parte, estão ok, mas tem alguns casos que não estão tão bons assim. Isso pode ser por causa da quantidade de palavras que temos no dicionario, que é relativamente pequena, ou por causa da estratégia de escolha de letras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
